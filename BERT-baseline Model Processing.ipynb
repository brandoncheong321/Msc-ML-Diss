{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import copy\n",
    "import spacy\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard loading train set:\n",
    "os.chdir('/Users/brandoncheong/Desktop/ML_Dissertation/wiki-pages')\n",
    "full_train_data_new = np.load('full_train_data_new.npy', allow_pickle = 'True')\n",
    "full_dev_data_new = np.load('full_dev_data_new.npy', allow_pickle = 'True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NER = spacy.load(\"en_core_web_sm\")\n",
    "unmasker = pipeline('fill-mask', model='bert-large-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, begin processing non-standard train_data using Bert_large:\n",
    "\n",
    "\n",
    "# obtain full dataset consisting of original claims + Bert-generated evidence_statements:\n",
    "count = 0 \n",
    "bert_train_data = list()\n",
    "for dictionary in full_train_data_new:\n",
    "           entry = copy.deepcopy(dictionary)\n",
    "           NER_on_claim = NER(entry['claim'])\n",
    "    # set all claims to 'empty' if not appropriate - will skip this in training data\n",
    "           if len(list(NER_on_claim.ents)) == 0:\n",
    "                entry['claim'] = 'Empty'\n",
    "                continue\n",
    "                \n",
    "            elif  len(list(NER_on_claim.ents)) == 1: # mask last word in claim\n",
    "            \n",
    "                masked_entity = masked_entity.split()[-2] # accounting for punctuation e.g full-stops\n",
    "                masked_entity_start_idx = entry['claim'].index(masked_entity)\n",
    "                text = entry['claim'][:masked_entity_start_idx] + '[MASK]' \n",
    "                if text[-1] == '.':\n",
    "                       pass\n",
    "                else:\n",
    "                       text += '.'\n",
    "                        \n",
    "                prediction = unmasker(text)[0]['token_str']\n",
    "                entry['evidence'] = entry['claim'][:masked_entity_start_idx] + prediction \n",
    "                bert_train_data.append(entry) \n",
    "                \n",
    "           else: \n",
    "                masked_entity = list(NER_on_claim.ents)[-1].text\n",
    "                masked_entity_start_idx = entry['claim'].index(masked_entity)\n",
    "                masked_entity_end_idx = masked_entity_start_idx + len(masked_entity) + 1\n",
    "                text = entry['claim'][:masked_entity_start_idx] + '[MASK]' + entry['claim'][masked_entity_end_idx:]\n",
    "                if text[-1] == '.':\n",
    "                       pass\n",
    "                else:\n",
    "                       text += '.'\n",
    "                print(text)\n",
    "                prediction = unmasker(text)[0]['token_str']\n",
    "                print(prediction)\n",
    "                entry['evidence'] = entry['claim'][:masked_entity_start_idx] + prediction + entry['claim'][masked_entity_end_idx:]\n",
    "                bert_train_data.append(entry) \n",
    "                print(f'Entry {count} done!')\n",
    "                count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, begin processing non-standard train_data using Bert_large:\n",
    "\n",
    "\n",
    "# obtain full dataset consisting of original claims + Bert-generated evidence_statements:\n",
    "count = 0 \n",
    "bert_dev_data = list()\n",
    "for dictionary in full_dev_data_new:\n",
    "           entry = copy.deepcopy(dictionary)\n",
    "           NER_on_claim = NER(entry['claim'])\n",
    "    # set all claims to 'empty' \n",
    "           if len(list(NER_on_claim.ents)) == 0:\n",
    "                entry['claim'] = 'Empty'\n",
    "                continue\n",
    "                \n",
    "            elif  len(list(NER_on_claim.ents)) == 1: # mask last word in claim\n",
    "            \n",
    "                masked_entity = masked_entity.split()[-2] # accounting for punctuation e.g full-stops\n",
    "                masked_entity_start_idx = entry['claim'].index(masked_entity)\n",
    "                text = entry['claim'][:masked_entity_start_idx] + '[MASK]' \n",
    "                if text[-1] == '.':\n",
    "                       pass\n",
    "                else:\n",
    "                       text += '.'\n",
    "                        \n",
    "                prediction = unmasker(text)[0]['token_str']\n",
    "                entry['evidence'] = entry['claim'][:masked_entity_start_idx] + prediction \n",
    "                bert_dev_data.append(entry) \n",
    "                \n",
    "           else: \n",
    "                masked_entity = list(NER_on_claim.ents)[-1].text\n",
    "                masked_entity_start_idx = entry['claim'].index(masked_entity)\n",
    "                masked_entity_end_idx = masked_entity_start_idx + len(masked_entity) + 1\n",
    "                text = entry['claim'][:masked_entity_start_idx] + '[MASK]' + entry['claim'][masked_entity_end_idx:]\n",
    "                if text[-1] == '.':\n",
    "                       pass\n",
    "                else:\n",
    "                       text += '.'\n",
    "                print(text)\n",
    "                prediction = unmasker(text)[0]['token_str']\n",
    "                print(prediction)\n",
    "                entry['evidence'] = entry['claim'][:masked_entity_start_idx] + prediction + entry['claim'][masked_entity_end_idx:]\n",
    "                bert_dev_data.append(entry) \n",
    "                print(f'Entry {count} done!')\n",
    "                count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
