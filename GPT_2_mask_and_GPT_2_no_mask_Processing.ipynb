{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GPT-2-mask and GPT-2-no-mask Processing.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7b2KnI2qRcB"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import copy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BiT5nFinqgSf"
      },
      "source": [
        "!pip install transformers\n",
        "from transformers import pipeline, set_seed\n",
        "from transformers import GPT2Tokenizer, GPT2Model\n",
        "\n",
        "import spacy\n",
        "from spacy import displacy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "de6BDKheqgUf"
      },
      "source": [
        "os.chdir('/content/drive/MyDrive/ML_Diss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10v1H_p9qgWs"
      },
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = TFGPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rH61YcX8qgZE"
      },
      "source": [
        "# Masking training and test sets:\n",
        "\n",
        "masked_train_data = copy.deepcopy(full_train_data_new)\n",
        "NER = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "for entry in masked_train_data:\n",
        "    NER_on_claim = NER(entry['claim'])\n",
        "    if len(list(NER_on_claim.ents)) == 0:\n",
        "        entry['claim'] = 'Empty'\n",
        "        continue\n",
        "    else: \n",
        "        masked_train_data = list(NER_on_claim.ents)[-1].text\n",
        "        masked_entity_idx = entry['claim'].index(masked_entity)\n",
        "        entry['claim'] = entry['claim'][:masked_entity_idx]\n",
        "        print(f'Entry {masked_train_data.index(entry)} done!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48AWq6kxrhqZ"
      },
      "source": [
        "masked_dev_data = copy.deepcopy(full_dev_data_new)\n",
        "NER = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "for entry in masked_dev_data:\n",
        "    NER_on_claim = NER(entry['claim'])\n",
        "    if len(list(NER_on_claim.ents)) == 0:\n",
        "        entry['claim'] = 'Empty'\n",
        "        continue\n",
        "    else: \n",
        "        masked_entity = list(NER_on_claim.ents)[-1].text\n",
        "        masked_entity_idx = entry['claim'].index(masked_entity)\n",
        "        entry['claim'] = entry['claim'][:masked_entity_idx]\n",
        "        print(f'Entry {masked_dev_data.index(entry)} done!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXc-77rfqgbc"
      },
      "source": [
        "# Begin with GPT-2-mask: \n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "full_GPT2_train_data = copy.deepcopy(masked_train_data)\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "count = 0\n",
        "\n",
        "for entry in full_GPT2_train_data:\n",
        "    prompt = entry['claim']\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors='tf')\n",
        "    candidate_sequences = list()\n",
        "    output = model.generate(\n",
        "       input_ids, \n",
        "       max_length=50, \n",
        "       num_beams=5, num_return_sequences = 5,\n",
        "       early_stopping=True)\n",
        "       for i in range(5):\n",
        "          sequence = tokenizer.decode(output[i], skip_special_tokens=True)\n",
        "          candidate_sequences.append(candidate_sequences)\n",
        "    reference = [prompt.split()]      \n",
        "    bleu_scores = [sentence_bleu(reference, candidate.split()) for candidate in candidate_sequences]\n",
        "    ev_statement = candidate_sequences[bleu_scores.index(max(bleu_scores))]\n",
        "    entry['evidence'] = ev_statement\n",
        "    print(f'Done with {count}th entry')\n",
        "    # save to Google Drive folder after every 100 entries:\n",
        "    if count%100 == 0:\n",
        "      np.save('full_GPT2_train_data.npy', full_GPT2_dev_data) \n",
        "      print('Saved training set!')\n",
        "    count += 1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzC9MQfIqgeC"
      },
      "source": [
        "full_GPT2_dev_data = copy.deepcopy(masked_dev_data)\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "count = 0\n",
        "\n",
        "for entry in full_GPT2_dev_data:\n",
        "    prompt = entry['claim']\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors='tf')\n",
        "    candidate_sequences = list()\n",
        "    output = model.generate(\n",
        "       input_ids, \n",
        "       max_length=50, \n",
        "       num_beams=5, num_return_sequences = 5,\n",
        "       early_stopping=True)\n",
        "       for i in range(5):\n",
        "          sequence = tokenizer.decode(output[i], skip_special_tokens=True)\n",
        "          candidate_sequences.append(candidate_sequences)\n",
        "    reference = [prompt.split()]      \n",
        "    bleu_scores = [sentence_bleu(reference, candidate.split()) for candidate in candidate_sequences]\n",
        "    ev_statement = candidate_sequences[bleu_scores.index(max(bleu_scores))]\n",
        "    entry['evidence'] = ev_statement\n",
        "    print(f'Done with {count}th entry')\n",
        "    # save to Google Drive folder after every 100 entries:\n",
        "    if count%100 == 0:\n",
        "      np.save('full_GPT2_train_data.npy', full_GPT2_dev_data) \n",
        "      print('Saved training set!')\n",
        "    count += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wdh7TIp0zBbU"
      },
      "source": [
        "# Now, we process GPT-2-no-mask:\n",
        "\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "count = 0\n",
        "full_GPT2_train_data_no_mask = list()\n",
        "\n",
        "for entry in full_GPT2_train_data:\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors='tf')\n",
        "    candidate_sequences = list()\n",
        "    output = model.generate(\n",
        "       input_ids, \n",
        "       max_length=50, \n",
        "       num_beams=5, num_return_sequences = 5,\n",
        "       early_stopping=True)\n",
        "       for i in range(5):\n",
        "          sequence = tokenizer.decode(output[i], skip_special_tokens=True)\n",
        "          candidate_sequences.append(candidate_sequences)\n",
        "    reference = [prompt.split()]      \n",
        "    bleu_scores = [sentence_bleu(reference, candidate.split()) for candidate in candidate_sequences]\n",
        "    ev_statement = candidate_sequences[bleu_scores.index(max(bleu_scores))]\n",
        "    entry['evidence'] = ev_statement\n",
        "    print(f'Done with {count}th entry')\n",
        "    # save to Google Drive folder after every 100 entries:\n",
        "    if count%100 == 0:\n",
        "      np.save('full_GPT2_train_data_no_mask.npy', full_GPT2_train_data_no_mask) \n",
        "      print('Saved training set!')\n",
        "    count += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXnaKY95y1N2"
      },
      "source": [
        "set_seed(42)\n",
        "\n",
        "count = 0\n",
        "full_GPT2_dev_data_no_mask = list()\n",
        "\n",
        "for entry in full_GPT2_dev_data:\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors='tf')\n",
        "    candidate_sequences = list()\n",
        "    output = model.generate(\n",
        "       input_ids, \n",
        "       max_length=50, \n",
        "       num_beams=5, num_return_sequences = 5,\n",
        "       early_stopping=True)\n",
        "       for i in range(5):\n",
        "          sequence = tokenizer.decode(output[i], skip_special_tokens=True)\n",
        "          candidate_sequences.append(candidate_sequences)\n",
        "    reference = [prompt.split()]      \n",
        "    bleu_scores = [sentence_bleu(reference, candidate.split()) for candidate in candidate_sequences]\n",
        "    ev_statement = candidate_sequences[bleu_scores.index(max(bleu_scores))]\n",
        "    entry['evidence'] = ev_statement\n",
        "    print(f'Done with {count}th entry')\n",
        "    # save to Google Drive folder after every 100 entries:\n",
        "    if count%100 == 0:\n",
        "      np.save('full_GPT2_dev_data_no_mask.npy', full_GPT2_dev_data_no_mask) \n",
        "      print('Saved test set!')\n",
        "    count += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmgOHGZGy1QN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKPMG2UIy1Sc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0P1HlLdy1VE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}