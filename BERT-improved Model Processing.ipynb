{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import copy\n",
    "import spacy\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard loading train set:\n",
    "os.chdir('/Users/brandoncheong/Desktop/ML_Dissertation/wiki-pages')\n",
    "full_train_data_new = np.load('full_train_data_new.npy', allow_pickle = 'True')\n",
    "full_dev_data_new = np.load('full_dev_data_new.npy', allow_pickle = 'True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NER = spacy.load(\"en_core_web_sm\")\n",
    "unmasker = pipeline('fill-mask', model='bert-large-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain full dataset consisting of original claims + Bert-generated evidence_statements:\n",
    "\n",
    "bert_modified_train_data = list()\n",
    "count = 0 \n",
    "for dictionary in full_train_data_new:\n",
    "           entry = copy.deepcopy(dictionary)\n",
    "           NER_on_claim = NER(entry['claim'])\n",
    "    # set all claims to 'empty' if not appropriate - will skip this in training data\n",
    "           if len(list(NER_on_claim.ents)) == 0:\n",
    "                entry['claim'] = 'Empty'\n",
    "                continue\n",
    "           elif  len(list(NER_on_claim.ents)) == 1: # mask last word in claim\n",
    "            \n",
    "                masked_entity = masked_entity.split()[-2] # accounting for punctuation e.g full-stops\n",
    "                masked_entity_start_idx = entry['claim'].index(masked_entity)\n",
    "                text = entry['claim'][:masked_entity_start_idx] + '[MASK]' \n",
    "                if text[-1] == '.':\n",
    "                       pass\n",
    "                else:\n",
    "                       text += '.'\n",
    "                        \n",
    "                prediction = unmasker(text)[0]['token_str']\n",
    "                entry['evidence'] = entry['claim'][:masked_entity_start_idx] + prediction \n",
    "                bert_modified_train_data.append(entry) \n",
    "            \n",
    "            \n",
    "           else: \n",
    "                masked_entity = list(NER_on_claim.ents)[-1]\n",
    "                # MODIFIED: ADD VARIABLE TRACKING MASKED_ENTITY TYPE:\n",
    "                masked_entity_label = masked_entity.label_\n",
    "                masked_entity = masked_entity.text\n",
    "                masked_entity_start_idx = entry['claim'].index(masked_entity)\n",
    "                masked_entity_end_idx = masked_entity_start_idx + len(masked_entity) + 1\n",
    "                text = entry['claim'][:masked_entity_start_idx] + '[MASK]' + entry['claim'][masked_entity_end_idx:]\n",
    "             \n",
    "                if text[-1] == '.':\n",
    "                       pass\n",
    "                else:\n",
    "                       text += '.'\n",
    "                print(text)\n",
    "                # MODIFIED: MODIFIED PREDICTION PROCEDURE TO ACCOUNT FOR MATCHING ENTITY LABEL:\n",
    "                claim_entities_list = [entity['token_str'] for entity in unmasker(text)]\n",
    "                claim_entities_labels_list = [NER(entity).ents[0].label_ if len(NER(entity).ents) != 0 else 0 for entity in claim_entities_list]\n",
    "                # cover edge case where claim_entities_labels_list has all empty lists:\n",
    "                if claim_entities_labels_list == [0 for i in range(len(claim_entities_list))] :\n",
    "                    prediction = claim_entities_list[0]\n",
    "                else:\n",
    "                    if masked_entity_label in claim_entities_labels_list:\n",
    "                        prediction = claim_entities_list[claim_entities_labels_list.index(masked_entity_label)]\n",
    "                    else:\n",
    "                        first_entity_idx = [idx for idx, ent_type in enumerate(claim_entities_labels_list) if ent_type != 0][0]\n",
    "                        prediction = claim_entities_list[first_entity_idx]\n",
    "                        # predicts legit entity deemed most probable by Bert\n",
    "            \n",
    "                \n",
    "                print(prediction)\n",
    "                entry['evidence'] = entry['claim'][:masked_entity_start_idx] + prediction + entry['claim'][masked_entity_end_idx:]\n",
    "                bert_modified_train_data.append(entry) \n",
    "                print(f'Entry {count} done!')\n",
    "                count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain full dataset consisting of original claims + Bert-generated evidence_statements:\n",
    "\n",
    "bert_modified_dev_data = list()\n",
    "count = 0 \n",
    "for dictionary in full_dev_data_new:\n",
    "           entry = copy.deepcopy(dictionary)\n",
    "           NER_on_claim = NER(entry['claim'])\n",
    "    # set all claims to 'empty' if not appropriate - will skip this in training data\n",
    "           if len(list(NER_on_claim.ents)) == 0:\n",
    "                entry['claim'] = 'Empty'\n",
    "                continue\n",
    "           elif  len(list(NER_on_claim.ents)) == 1: # mask last word in claim\n",
    "            \n",
    "                masked_entity = masked_entity.split()[-2] # accounting for punctuation e.g full-stops\n",
    "                masked_entity_start_idx = entry['claim'].index(masked_entity)\n",
    "                text = entry['claim'][:masked_entity_start_idx] + '[MASK]' \n",
    "                if text[-1] == '.':\n",
    "                       pass\n",
    "                else:\n",
    "                       text += '.'\n",
    "                        \n",
    "                prediction = unmasker(text)[0]['token_str']\n",
    "                entry['evidence'] = entry['claim'][:masked_entity_start_idx] + prediction \n",
    "                bert_modified_train_data.append(entry) \n",
    "            \n",
    "            \n",
    "           else: \n",
    "                masked_entity = list(NER_on_claim.ents)[-1]\n",
    "                # MODIFIED: ADD VARIABLE TRACKING MASKED_ENTITY TYPE:\n",
    "                masked_entity_label = masked_entity.label_\n",
    "                masked_entity = masked_entity.text\n",
    "                masked_entity_start_idx = entry['claim'].index(masked_entity)\n",
    "                masked_entity_end_idx = masked_entity_start_idx + len(masked_entity) + 1\n",
    "                text = entry['claim'][:masked_entity_start_idx] + '[MASK]' + entry['claim'][masked_entity_end_idx:]\n",
    "             \n",
    "                if text[-1] == '.':\n",
    "                       pass\n",
    "                else:\n",
    "                       text += '.'\n",
    "                print(text)\n",
    "                # MODIFIED: MODIFIED PREDICTION PROCEDURE TO ACCOUNT FOR MATCHING ENTITY LABEL:\n",
    "                claim_entities_list = [entity['token_str'] for entity in unmasker(text)]\n",
    "                claim_entities_labels_list = [NER(entity).ents[0].label_ if len(NER(entity).ents) != 0 else 0 for entity in claim_entities_list]\n",
    "                # cover edge case where claim_entities_labels_list has all empty lists:\n",
    "                if claim_entities_labels_list == [0 for i in range(len(claim_entities_list))] :\n",
    "                    prediction = claim_entities_list[0]\n",
    "                else:\n",
    "                    if masked_entity_label in claim_entities_labels_list:\n",
    "                        prediction = claim_entities_list[claim_entities_labels_list.index(masked_entity_label)]\n",
    "                    else:\n",
    "                        first_entity_idx = [idx for idx, ent_type in enumerate(claim_entities_labels_list) if ent_type != 0][0]\n",
    "                        prediction = claim_entities_list[first_entity_idx]\n",
    "                        # predicts legit entity deemed most probable by Bert\n",
    "            \n",
    "                \n",
    "                print(prediction)\n",
    "                entry['evidence'] = entry['claim'][:masked_entity_start_idx] + prediction + entry['claim'][masked_entity_end_idx:]\n",
    "                bert_modified_dev_data.append(entry) \n",
    "                print(f'Entry {count} done!')\n",
    "                count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('bert_modified_train_data.npy', bert_modified_train_data)\n",
    "np.save('bert_modified_dev_data.npy', bert_modified_dev_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
